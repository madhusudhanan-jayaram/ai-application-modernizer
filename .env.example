# Local LLM Configuration (Ollama)
OLLAMA_BASE_URL=http://localhost:11434
LLM_PROVIDER=ollama
MODEL_NAME=qwen2.5-coder:7b
OLLAMA_NUM_PREDICT=2048
OLLAMA_TEMPERATURE=0.1

# GitHub Configuration (optional for private repos)
GITHUB_TOKEN=your_github_token_here

# Application Settings
DATA_DIR=./data
LOG_LEVEL=INFO
MAX_FILES_TO_ANALYZE=50
MAX_FILE_SIZE_MB=5
CONTEXT_WINDOW_TOKENS=32000
CHUNK_OVERLAP_TOKENS=500

# Performance & Caching
ENABLE_CACHING=true
CACHE_TTL_HOURS=24

# Streamlit Configuration (optional)
STREAMLIT_SERVER_PORT=8501
STREAMLIT_SERVER_HEADLESS=false
